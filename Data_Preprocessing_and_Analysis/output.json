{
    "content": "FeTrIL: Feature Translation for Exemplar-Free Class-Incremental Learning\nGr\u00b4egoire Petit1,2, Adrian Popescu1, Hugo Schindler1, David Picard2, Bertrand Delezoide3\n1Universit \u00b4e Paris-Saclay, CEA, LIST, F-91120, Palaiseau, France\n2LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, Marne-la-Vall \u00b4ee, France\n3Amanda, 34 Avenue Des Champs Elys \u00b4ees, F-75008, Paris, France\ng.petit360@gmail.com, adrian.popescu@cea.fr,hugo-schindler@orange.fr\ndavid.picard@enpc.fr,bertrand.delezoide@amanda.com\nAbstract\nExemplar-free class-incremental learning is very chal-\nlenging due to the negative effect of catastrophic forget-\nting. A balance between stability and plasticity of the in-\ncremental process is needed in order to obtain good accu-\nracy for past as well as new classes. Existing exemplar-free\nclass-incremental methods focus either on successive fine\ntuning of the model, thus favoring plasticity, or on using\na feature extractor fixed after the initial incremental state,\nthus favoring stability. We introduce a method which com-\nbines a fixed feature extractor and a pseudo-features gen-\nerator to improve the stability-plasticity balance. The gen-\nerator uses a simple yet effective geometric translation of\nnew class features to create representations of past classes,\nmade of pseudo-features. The translation of features only\nrequires the storage of the centroid representations of past\nclasses to produce their pseudo-features. Actual features\nof new classes and pseudo-features of past classes are fed\ninto a linear classifier which is trained incrementally to dis-\ncriminate between all classes. The incremental process is\nmuch faster with the proposed method compared to main-\nstream ones which update the entire deep model. Experi-\nments are performed with three challenging datasets, and\ndifferent incremental settings. A comparison with ten exist-\ning methods shows that our method outperforms the oth-\ners in most cases. FeTrIL code is available at https:\n//github.com/GregoirePetit/FeTrIL .\n1. Introduction\nDeep learning [8] has dramatically improved the qual-\nity of automatic visual recognition, both in terms of ac-\ncuracy and scale. Current models discriminate between\nthousands of classes with an accuracy often close to that\nof human recognition, assuming that sufficient training ex-\namples are provided. Unlike humans, algorithms reachoptimal performance only if trained with all data at once\nwhenever new classes are learned. This is an important\nlimitation because data often occur in sequences [18] and\ntheir storage is costly. Also, iterative retraining to integrate\nnew data is computationally costly and difficult in time- or\ncomputation-constrained applications [10, 33]. Incremen-\ntal learning [37] was introduced to reduce the memory and\ncomputational costs of machine learning algorithms. The\nmain problem faced by class-incremental learning (CIL)\nmethods is catastrophic forgetting [15, 26], the tendency\nof neural nets to underfit past classes when ingesting new\ndata. Many recent solutions [4, 14, 34, 45, 47], based\non deep nets, use replay from a bounded memory of the\npast to reduce forgetting. However, replay-based methods\nmake a strong assumption because past data are often un-\navailable [42]. Also, the footprint of the image memory\ncan be problematic for memory-constrained devices [33].\nExemplar-free class-incremental learning (EFCIL) methods\nrecently gained momentum [46, 39, 48, 49]. Most of them\nuse distillation [13] to preserve past knowledge, and gener-\nally favor plasticity. New classes are well predicted since\nmodels are learned with all new data and only a representa-\ntion of past data [25, 32, 50]. A few EFCIL methods [1, 6]\nare inspired by transfer learning [38, 40]. They learn a fea-\nture extractor in the initial state, and use it as such later to\ntrain new classifiers. In this case, stability is favored over\nplasticity since the model is frozen [25].\nWe introduce FeTrIL, a new EFCIL method which com-\nbines a frozen feature extractor and a pseudo-feature gen-\nerator to improve incremental performance. New classes\nare represented by their image features obtained from the\nfeature extractor. Past classes are represented by pseudo-\nfeatures which are derived from features of new classes\nby using a geometric translation process. This translation\nmoves features toward a region of the features space which\nis relevant for past classes. The proposed pseudo-feature\ngeneration is adapted for EFCIL since it is simple, fast and\nonly requires the storage of the centroids for past classes.arXiv:2211.13131v2  [cs.CV]  28 Nov 2023Initial state\nf(C1)f(C2)f(C3)(a)\nIncremental state 1\nf1(C1)\nf1(C2)\nf1(C3)\nf(C4) (b)\nIncremental state 2\nf2(C1)\nf2(C2)\nf2(C3)\nf2(C4)\nf(C5) (c)\nActual features\nf(C1)\nf(C2)f(C3)\nf(C4)f(C5) (d)\nFigure 1. Illustration of the proposed pseudo-feature generation procedure. This toy example includes an initial state (3 classes) and two IL\nstates (1 new class per state) in subfigures (a), (b) and (c). Subfigure (d) provides the actual features of all classes that would be available\nfor a classical learning. The illustration uses a 2D projection of actual features. Pseudo-features of past classes are generated by geometric\ntranslation of features of the new class added in each state with the difference between the centroids of the target past class and of the new\nclass. While imperfect, the pseudo-feature generator produces a usable representation of past classes. Best viewed in color.\nFeTrIL is illustrated with a toy example in Figure 1. We\nrun experiments with a standard EFCIL setting [14, 48, 49],\nwhich consists of a larger initial state, followed by smaller\nstates which include the same number of classes. Results\nshow that the proposed approach has better behavior com-\npared to ten existing methods, including very recent ones.\n2. Related Work\nCIL algorithms are needed when data arrive sequentially\nand/or computational constraints are important [10, 18, 25,\n30]. Their objective is to ensure a good balance between\nplasticity, i.e. integration of new information, and stability,\ni.e. preservation of knowledge about past classes [28]. This\nis challenging because the lack of past data leads to catas-\ntrophic forgetting, i.e. the tendency of neural networks to\nfocus on newly learned data at the expense of past knowl-\nedge [26]. Recent reviews of CIL [2, 25] show that a ma-\njority of methods replay samples of past classes to mitigate\nforgetting [4, 14, 34, 47]. One advantage here is that the\nnetwork architecture remains constant throughout the incre-\nmental process. However, these methods have two major\ndrawbacks: (1) First, the assumption that past samples are\navailable is strong since in many cases past data cannot be\nstored due, for instance, to privacy restrictions [42] and (2)\nthe memory footprint of the stored images is high.\nHere, we investigate exemplar-free CIL, with focus on\nmethods which keep the network size constant. This setting\nis very challenging since it imposes strong constraints on\nboth memory and computational costs. A majority of ex-\nisting methods use regularization to update the deep model\nfor each incremental step [25], and adapt distillation [13] to\npreserve past knowledge by penalizing variations for past\nclasses during model updates. Note that, while some of the\ndistillation-based methods were introduced in an exemplar-\nbased CIL (EBCIL) setting, many of them are also appli-\ncable to EFCIL. This approach to CIL was popularizedby iCaRL [34], itself inspired by learning without forget-\nting (LwF) [20]. Distillation was later refined and comple-\nmented with other components to improve the plasticity-\nstability compromise. LUCIR [14] applies distillation on\nfeatures instead of raw classification scores to preserve the\ngeometry of past classes, and an inter-class separation to\nmaximize the distances between past and new classes. The\nproblem was partially addressed by adding specific class\nseparability components in [7, 14]. Distillation-based\nmethods need to store the current and the preceding model\nfor incremental updates. Their memory footprint is larger\ncompared to methods which do not use distillation [25].\nAnother important problem in CIL is the semantic drift\nbetween incremental states. Auxiliary classifiers were intro-\nduced in [21] to reduce the effect of forgetting. ABD [39]\nuses image inversion to produce pseudo-samples of past\nclasses. The method is interesting but image inversion is\ndifficult for complex datasets. Another interesting solution\nis proposed in [46], where the features drift between incre-\nmental steps is estimated from that of new classes. Recent\nEFCIL approaches [48, 49, 50] use past class prototypes in\nconjunction with distillation to improve performance. Pro-\ntotype augmentation is proposed in PASS [49] to improve\nthe discrimination of classes learned in different incremen-\ntal states. Feature generation for past classes is introduced\nin IL2A [48] by leveraging information about the class dis-\ntribution. This approach is difficult to scale-up because a\ncovariance matrix needs to be stored for each class. A pro-\ntotype selection mechanism is introduced in SSRE [50] to\nbetter discriminate past from new classes. FeTrIL shares the\nidea of using class prototypes with [46, 48, 49, 50]. An im-\nportant difference is that we freeze the model after the initial\nstate, while the other methods deploy more sophisticated\nmechanisms to integrate prototypes in a knowledge distilla-\ntion process. Past comparative studies [2, 25] found that,\nwhile appealing in theory, distillation-based methods un-derperform in EFCIL, particularly for large-scale datasets.\nSecond, since the representation space is fixed, a simple\ngeometric translation of actual features of new classes is\nsufficient to produce usable pseudo-features. In contrast,\nIL2A [48], the work which is closest to ours, needs to store a\ncovariance matrix per class to obtain optimal performance.\nThird, the use of a fixed extractor simplifies the training pro-\ncess since only the final linear layer is trained, compared\nto a fine tuning of the backbone model required by recent\nmethods which use prototypes and feature generation.\nAnother line of work takes inspiration from transfer\nlearning [29, 38] to tackle EFCIL. A feature extractor is\ntrained in the initial non-incremental state and fixed after-\nwards. Then, an external classification layer is updated in\neach incremental state to integrate new classes. The nearest\nclass mean (NCM) [27] was used in [34], linear SVMs [31]\nwere used in [1] and extreme value machines [35] were re-\ncently tested by [6]. The advantages of transfer-learning\nmethods are their simplicity, since only the classification\nlayer is updated, and their lower memory requirement, since\nthey need a single deep model to function. These meth-\nods give competitive performance compared to distillation-\nbased ones in EFCIL, particularly at scale [2]. However,\nfeatures are not updated, and they are sensitive to large do-\nmain shifts between incremental tasks [18]. Equally, exist-\ning transfer-learning inspired works do not sufficiently ad-\ndress inter-class separability, which is in focus here.\nClass prototypes creation was studied in other learning\nsettings than CIL. A very interesting method focused on\nfew-shot learning was proposed in [5]. A distance-based\nclassifier which uses an approximation of the Mahalanobis\ndistance is proposed. The means and variances of new\nclasses are predicted using two supplementary neural net-\nworks. While adapted for few-shot learning, such an ap-\nproach is not fully adapted in CIL. First, the supplementary\nneural networks require a large number of supplementary\nparameters. This is a disadvantage here, since CIL methods\nare needed in computationally-constrained environments.\nSecond, we do not focus on few-shot learning and the means\nof past classes are well-placed in the representation space.\n3. Proposed Method\nThe objective of CIL is to learn a total of Nclasses\nwhich appear sequentially during training. This process\nincludes an initial state (0) and Tincremental ones. New\nclasses need to be recognized alongside past classes which\nwere learned in previous states. We focus on the exemplar-\nfree CIL setting [34, 39, 46, 50], which assumes that no\npast images can be stored. This scenario is more challeng-\ning than exemplar-based CIL since catastrophic forgetting\nneeds to be tackled without resorting to replay [25]. There\nis no intersection between the classes learned in different\nincremental states. Unlike task IL [41], the boundaries be-tween different states are not known at test time.\nThe global functioning of FeTrIL is illustrated in Fig-\nure 2. It uses a feature extractor, a pseudo-feature generator\nbased on geometric translation, and an external classifica-\ntion layer in order to address EFCIL. Inspired by transfer-\nlearning based CIL [1, 34], the feature extractor Fis frozen\nafter the initial state. This ensures a stable representation\nspace through the entire CIL process. Given that images\nof past classes cannot be stored in EFCIL, a generator Gis\nused to produce pseudo-features of past classes ( \u02c6ft(Cp)).G\ntakes features of new classes ( f(Cn)) and prototypes of past\nand new classes ( \u00b5(Cp),\u00b5(Cn)) as inputs. A linear classi-\nfierLcombines features and pseudo-features to jointly train\nclassifiers for all seen classes (past and new). The pseudo-\nfeatures generation is crucial since it enables class discrim-\nination across all incremental states. The hypotheses made\nhere are that: (1) while imperfect, the pseudo-features still\nproduce effective representations of past classes, and (2) us-\ning a frozen extractor in combination with a generator in\nEFCIL is preferable to mainstream distillation-based meth-\nods [46, 48, 49, 50]. These hypotheses are tested through\nthe extensive experiments in Section 4. We present the main\ncomponents of FeTrIL in the next subsections.\n3.1. Generation of pseudo-features\nThe pseudo-feature generator, illustrated in Figure 1,\nproduces effective representations of past classes. Exist-\ning approaches which generate past data rely on methods\nsuch as generative adversarial networks [11], image inver-\nsion [39], or covariance-based past class models [48]. We\npropose a much simpler alternative which is defined as:\n\u02c6ft(cp) =f(cn) +\u00b5(Cp)\u2212\u00b5(Cn) (1)\nwith: Cp- target past class for which pseudo-features are\nneeded; Cn- new class for which images bare available;\nf(cn)- features of a sample cnof class Cnextracted with\nF;\u00b5(Cp), \u00b5(Cn)- mean features of classes CpandCnex-\ntracted with F;\u02c6ft(cp)- pseudo-feature vector of a pseudo-\nsample cpof class Cpproduced in the tthincremental state.\nEq. 1 translates the value of each dimension with the dif-\nference between the values of the corresponding dimension\nof\u00b5(Cp)and\u00b5(Cn). It creates a pseudo-feature vector sit-\nuated in the region of the representation space associated\nto target class Cpbased on actual features of a new class\nf(Cn). The computational cost of generation is very small\nsince it only involves additions and subtractions. \u00b5(Cp)is\nneeded to drive the geometric translation toward a region of\nthe representation space which is relevant for Cp. Centroids\nare computed when classes occur for the first time and then\nstored. Their reuse is possible because Fis fixed after the\ninitial step and its associated features do not evolve.Figure 2. FeTrIL overview for a toy example with an initial state (3 classes) and two incremental states (1 class per state). The feature\nextractor Fis trained in the initial state, using sets of data X1, X2, X3, and then frozen afterwards. The generator Guses features f(Cn)\nof the new class extracted with Fand prototypes of past classes \u00b5(Cp)to generate pseudo-features of past classes \u02c6ft(Cp)in the tthstate.\nPrototypes ( \u00b5(Ci)) are the centroids of all classes (past and new). They are learned when classes are first seen and then stored throughout\nthe IL process. A linear classifier Lis used to learn classification weights w(Ci)for all seen classes (past and new).\n3.2. Selection of pseudo-features\nEq. 1 translates the features for a single sample. If each\nclass is represented by ssamples, the generation process\nneeds to be repeated stimes. The overview of FeTrIL (Fig-\nure 2) and of the pseudo-feature generation (Figure 1) use\na minimal example which adds a single class per IL state.\nWhen CIL states include several classes Cn, thespseudo-\nfeatures of each class Cpcan be obtained using different\nstrategies, depending on how features of new classes are\nused. We deploy the following strategies:\n\u2022 FeTrILk:sfeatures are transferred from the kthsimilar\nnew class of each past class Cp. Similarities between the\ntarget Cpand the Cnavailable in the current state is com-\nputed using the cosine similarity between the centroids of\neach pair of classes. Experiments are run with different\nvalues of kto assess if a variable class similarity has a sig-\nnificant effect on EFCIL performance. Since translation\nis based on a single new class, the distribution of pseudo-\nfeatures will be similar to that of features of Cn, but in\nthe region of the representation space around \u00b5(Cp).\n\u2022 FeTrILrand:sfeatures are randomly selected from all\nnew classes. This strategy assesses whether a more di-\nversified source of features from different Cnproduces\nan effective representation of class Cp.\n\u2022 FeTrILherd:sfeatures are selected from any new class\nbased on a herding algorithm [44]. It assumes that sam-\npling should include features which produce a good ap-\nproximation of the past class. Herding was introduced\nin exemplar-based CIL in order to obtain an accurate\napproximation of each class by using only a few sam-\nples [34] and its usefulness was later confirmed [2, 14,\n45]. It is adapted here to obtain a good approximation of\nthe sample distribution of Cpwithspseudo-features.\nThe comparison of these different strategies will allow us\nto determine whether the geometric translation of features\nis prevalent, or if a particular configuration of the featuresaround the centroid of the target past class is needed.\n3.3. Linear classification layer training\nWe assume that the CIL process is in the tthCIL state,\nwhich includes Ppast classes and Nnew classes. The com-\nbination of the feature generator (Subsection 3.1) and se-\nlection (Subsection 3.2) provides a set \u02c6ft(Cp)ofspseudo-\nfeatures for each class Cp. The objective is to train a linear\nclassifier for all P+Nseen classes which takes pseudo\nfeatures of past classes and actual features of new classes as\ninputs. This linear layer is defined as:\nWt={wt(C1), ..., wt(CP), wt(CP+1), ..., wt(CP+N)}(2)\nwith: wt- the weight of known classes in the tthCIL state.\nWtcan be implemented using different classifiers, and\nwe instantiate two versions in Section 4: (1) FeTrIL using\nLinearSVCs [31] as external classifiers, and (2) FeTrIL fc\nusing a fully-connected layer to enable end-to-end training.\n4. Evaluation\nWe evaluate FeTrIL by using a comprehensive EFCIL\nevaluation scenario [48, 49, 50]. This setting includes four\ndatasets and CIL states of different size.\nDatasets. We use four public datasets: (1) CIFAR-\n100 [17] - 100 classes, 32x32 pixels images, 500 and 100\nimages/class for training and test; (2) TinyImageNet [19] -\n200 leaf clases from ImageNet, 64x64 pixels images, 500\nand 50 for training and test; (3) ImageNet-Subset - 100\nclasses subset of ImageNet LSVRC dataset [36], 1300 and\n50 for training and test; (4) ILSVRC - full dataset from [36].\nIncremental setting. We use a classical EFCIL protocol\nfrom [48, 49, 50]. The number of classes in the initial state\nis larger, and the rest of the classes are evenly distributed\nbetween incremental states. CIFAR-100 and ImageNet-\nSubset are tested with: (1) 50 initial classes and 5 IL states\nof 10 classes, (2) 50 initial classes and 10 IL states of 5classes, (3) 40 initial classes and 20 states of 3 classes, and\n(4) 40 initial classes and 60 states of 1 class. Compared\nto [48, 49, 50], configurations (1) and (3) for ImageNet-\nSubset are added for more consistent evaluation. TinyIma-\ngeNet is tested with 100 initial classes and the other classes\ndistributed as follows: (1) 5 states of 20 classes, (2) 10\nstates of 10 classes, (3) 20 states of 5 classes, and (4) 100\nstates of 1 class. Configuration (4) is interesting since it en-\nables one class increments. It cannot be deployed for any\nof the compared EFCIL methods since they require at least\ntwo classes per increment to update models. ILSVRC is\ntested with 500 initial classes, and the other 500 split evenly\namong T\u2208 {5,10,20}states. This enables a comprehen-\nsive comparison of the methods in varied EFCIL configura-\ntions. Naturally, task IDs are not available at test time.\nCompared methods. We use the following EF-\nCIL methods in evaluation: EWC [16], LwF-MC [34],\nDeeSIL [1], LUCIR [14], MUC [21], SDC [46], PASS [49],\nABD [39], IL2A [48], SSRE [50]. As we discussed in Sec-\ntion 2, these methods cover a large variety of EFCIL ap-\nproaches. The inclusion of recent works [48, 49, 50] is im-\nportant to situate our contribution with respect to current\nEFCIL trends. While focus is on EFCIL, we follow [50]\nand include a comparisonwith EBCIL methods. We test\nour method against the recent AANets approach [22], and\nagainst the EBCIL methods to which AANETS was added\n(LUCIR [14], Mnemonics [23], PODNet [7]). Whenever\navailable, results of compared methods marked with\u2217are\nreproduced either from their initial paper or from [50] for\nEFCIL or from [22] for EBCIL. The other results are re-\ncomputed using the original configurations of the methods.\nImplementation details. Following [34, 48, 49, 50], we\nuse ResNet-18 [12] in all experiments. FeTrIL initial train-\ning is done uniquely with images of initial classes to ensure\ncomparability with existing methods. The feature extractor\nis trained in the initial state and then frozen for the reminder\nof the IL process. We implement a supervised training with\ncross-entropy loss, SGD optimization, a batch size of 128,\nfor a total of 160 epochs. The initial learning rate is 0.1, and\nit is decayed by 0.1 after every 50 epochs. To ensure com-\nparability, classes are assigned to IL states using the same\nrandom seed as in the compared methods [14, 49, 48, 50].\nWe provide implementation details for the final layer\n(Eq. 2) introduced in Subsection 3.3. The hyperparameters\nof the classification layers were optimized on a pool of 50\nclasses selected randomly from ImageNet, but disjoint from\nILSVRC or ImageNet-Subset. L2-normalization is applied\nbefore the linear layer. The LinearSVC layer included in\nFeTrIL1uses 1.0 and 0.0001 for regularization and the tol-\nerance parameters. The number of samples is higher than\nthe dimensionality of the features, and we solve the pri-\nmal rather than the dual optimization problem. The clas-\nsifiers are then trained using a standard one against the restprocedure. In Subsection 4.2, we also test a one-vs-many\nstrategy to accelerate incremental updates. The second vari-\nant, FeTrIL1\nfc, using a fully-connected layer as final layer,\nand implements an end-to-end training strategy. FeTrIL1\nfcis\ntrained for 50 epochs with an initial learning rate of 0.1, 0.1\ndecay, and 10 epochs patience.\nEvaluation metric. The average incremental accuracy,\nwidely used in CIL [25, 34], is the main evaluation mea-\nsure. For comparability with [48, 49, 50], it is computed\nas the average accuracy of all states, including the initial\none. We equally provide per-state accuracy curves to have\na more detailed view of the accuracy evolution during the\nCIL process. Following [50], we run each configuration of\nFeTrIL three times and report the averaged results.\n4.1. Results\nComparison to existing EFCIL methods. The re-\nsults from Table 1 show that FeTrIL1outperforms all com-\npared methods in 11 tested configurations out of 12. It\nis also close to the best in the remaining one. The sec-\nond best results are obtained with the very recent SSRE\nmethod [50]. FeTrIL1and SSRE accuracies are close to\neach other for CIFAR-100, with relative differences be-\ntween 0.4 and -0.2. The performance gain brought by\nFeTrIL is of over 4 and 3 top-1 accuracy points for TinyIm-\nageNet and ImageNet-Subset, respectively. PASS [49] and\nIL2A [48], two other recent EFCIL methods, have lower av-\nerage performance. We note that EFCIL performance boost\nwas recently reported, with methods such as PASS, IL2A,\nSSRE. These methods combine knowledge distillation and\nsophisticated mechanisms for dealing with the stability-\nplasticity dilemma. In contrast, our method uses a fixed fea-\nture extractor and a lightweight pseudo-feature generator.\nFeTrIL only optimizes a linear classification layer, while\ncompared recent methods use backpropagation of the en-\ntire model, and need much more computational resources\nand time to perform the IL process. A more in-depth dis-\ncussion of complexity is proposed in Subsection 4.2. Per-\nformance of the ILSVRC dataset is also very interesting.\nDirect comparison to PASS or SSRE is impossible since\nthese methods were not tested at scale. However, we can\nsafely assume that FeTrIL1is better given PASS and SSRE\naccuracy for the simpler ImageNet-Subset. ILSVRC results\nshow that the simple method proposed here is effective for a\nhigh range of classes. Interestingly, ILSVRC performance\nis stabler compared to smaller datasets since the pool of new\nclasses available for pseudo-features generation is larger.\nComparison to a transfer-learning baseline.\nDeeSIL [1] is a simple application of transfer learn-\ning to EFCIL. It has no class separability mechanism across\ndifferent incremental states since classifiers are learned\nwithin each state. The need for global separability, included\nin FeTrIL, is shown by the comparison of short and longCIL MethodCIFAR-100 TinyImageNet ImageNet-Subset ImageNet\nT=5 T=10 T=20 T=60 T=5 T=10 T=20 T=100 T=5 T=10 T=20 T=60 T=5 T=10 T=20\nEWC\u2217[16] (PNAS\u201917) 24.5 21.2 15.9 x 18.8 15.8 12.4 x - 20.4 - x - - -\nLwF-MC\u2217[34] (CVPR\u201917) 45.9 27.4 20.1 x 29.1 23.1 17.4 x - 31.2 - x - - -\nLUCIR (CVPR\u201919) 51.2 41.1 25.2 x 41.7 28.1 18.9 x 56.8 41.4 28.5 x 47.4 37.2 26.6\nMUC\u2217[21] (ECCV\u201920) 49.4 30.2 21.3 x 32.6 26.6 21.9 x - 35.1 - x - - -\nSDC\u2217[46] (CVPR\u201920) 56.8 57.0 58.9 x - - - x - 61.2 - x - - -\nABD\u2217[39] (ICCV\u201921) 63.8 62.5 57.4 x - - - x - - - x - - -\nPASS\u2217[49] (CVPR\u201921) 63.5 61.8 58.1 x 49.6 47.3 42.1 x 64.4 61.8 51.3 x - - -\nIL2A\u2217[48] (NeurIPS\u201921) 66.0 60.3 57.9 x 47.3 44.7 40.0 x - - - x - - -\nSSRE\u2217[50] (CVPR\u201922) 65.9 65.0 61.7 x 50.4 48.9 48.2 x - 67.7 - x - - -\nDeeSIL [1] (ECCVW\u201918) 60.0 50.6 38.1 x 49.8 43.9 34.1 x 67.9 60.1 50.5 x 61.9 54.6 45.8\nDSLDA [9] (CVPRW\u201920) 64.0 63.8 60.8 60.5 53.1 52.9 52.8 52.6 71.3 71.2 71.0 70.8 64.0 63.8 63.6\nFeTrIL166.3 65.2 61.5 59.8 54.8 53.1 52.2 50.2 72.2 71.2 67.1 65.4 66.1 65.0 63.8\nTable 1. Average top-1 incremental accuracy in EFCIL with different numbers of incremental steps. FeTrIL1results are reported with\npseudo-features translated from the most similar new class. \u201d-\u201d cells indicate that results were not available (see supp. material for details).\n\u201dx\u201d cells indicate that the configuration is impossible for that method. Best results - in bold , second best - underlined .\nCIL processes. DeeSIL [1] performance is good for T= 5\nbecause each class is trained against enough other classes,\nbut drops significantly for T= 20 , when there are few\nnew classes. The important performance gain brought by\nFeTrIL highlights the importance of class separability.\nBehavior for minimal incremental updates. Com-\npared EFCIL methods can only be updated with a minimum\nof two classes per CIL state since they use discriminative\nclassifiers, which require both positive and negative sam-\nples. In practice, it is interesting to enable updates once\neach new class is available. This is possible with FeTrIL\nbecause pseudo-features can all originate from a single new\nclass. Results in the right columns of CIFAR-100, Tiny-\nImageNet and ImageNet-Subset from Table 1 show that the\naccuracy obtained in with one class increments is close to\nthat observed for T= 20 . This highlights the robustness of\nFeTrIL with respect to frequent updates.\nInfluence of the final classification layer.\nFeTrIL1compares favorably with FeTrIL1\nfc. LinearSVC\ngives better performance than a fully-connected layer,\nparticularly for a large number of incremental steps.\nHowever, FeTrIL1\nfcis also competitive, and outperforms\nexisting methods in a majority of configurations.\nDetailed view of accuracy. We illustrate the evolution\nof accuracy across incremental states in Figure 3 to com-\nplement the averaged results from Table 1. These detailed\nresults confirm the good behavior of the proposed method.\nThe evolution of accuracy for FeTrIL and SSRE is very sim-\nilar for CIFAR-100, FeTrIL method is better throughout the\nprocess for TinyImageNet, and also better than SSRE for\nthe first incremental states for ImageNet-Subset. The per-\nformance gain with respect to the other compared methods\nis much larger for all incremental states.\nComparison to exemplar-based CIL methods. This\ncomparison is interesting because EFCIL is a much more\nchallenging task than EBCIL [2, 25], and an important per-\nformance gap between the two was observed. This is intu-\nitive since the storage of images of past classes in EBCILCIL MethodCIFAR-100 ImageNet-Subset\nT= 5 T= 10 T= 5 T= 10\nLUCIR [14] (CVPR\u201919) 63.2 61.1 70.8 68.3\n+AAnets (CVPR\u201921) 66.7 65.3 72.6 69.2\nMnemonics [24] (CVPR\u201920) 63.3 62.3 72.6 71.4\n+AAnets (CVPR\u201921) 67.6 65.7 72.9 71.9\nPODNet [7] (ECCV\u201920) 64.8 63.2 75.5 74.3\n+AAnets (CVPR\u201921) 66.3 64.3 77.0 75.6\nFeTrIL166.3 65.2 71.9 70.8\nTable 2. Comparison of FeTrIL with the recent AANets\nmethod [22], applied on top of EBCIL baselines which store 20\nexemplars of past classes to mitigate catastrophic forgetting.\nmitigates catastrophic forgetting. Following [14, 22], a\nmemory of 20 images per class is allowed for all EBCIL\nmethods tested here. FeTrIL is better than all three base\nmethods to which AANets is applied for CIFAR-100. For\nImageNet-Subset, FeTrIL accuracy is better than LUCIR\u2019s,\nslightly behind that of Mnemonics [23] and approximately\n3.5 points lower than that of PODNet [7]. The performance\nof FeTrIL remains close that of EBCIL methods in a ma-\njority of cases even after the introduction of AANets. The\nresults from Table 2 indicate that, while still present, the gap\nbetween EFCIL and EBCIL methods is narrowing.\n4.2. Method analysis\nWe present an analysis of: (1) the selection strategies, (2)\nthe memory footprint of the methods, (3) the complexity of\nmodel updates, and (4) the stability-plasticity balance.\nPseudo-feature selection comparison. FeTrIL can use\nany past-new classes combination for translation. In Ta-\nble 3, we compare the selection strategies from Subsec-\ntion 3.2. Accuracy varies in a relatively small range for all\nstrategies, indicating that FeTrIL is robust to the way fea-\ntures of new classes are selected, and it can be successfully\nimplemented with any of the strategies. FeTrIL1is better\nthan the other selection methods and this motivates its use\nin the main experiments. Class similarity matters, but re-\nsults with FeTrIL10remain interesting. FeTrILherdalso has\ninteresting accuracy, but is slightly behind that of FeTrIL1.012345678910\nIncremental state0%20%40%60%80%100%\nCIFAR-100, T=10\nPASS\nLUCIRMUC\nLwF-MCSSRE\nDeeSILIL2A\nFeTrIL\n012345678910\nIncremental state0%20%40%60%80%100%\nTinyImageNet, T=10\nPASS\nLUCIRMUC\nLwF-MCSSRE\nDeeSILIL2A\nFeTrIL\n012345678910\nIncremental state0%20%40%60%80%100%\nImageNet-Subset, T=10\nPASS\nLUCIRMUC\nLwF-MCSSRE\nDeeSILFeTrILFigure 3. Evolution of top-1 accuracy for an incremental process with T= 10 IL states. Best viewed in color.\nCIFAR-100 TinyImageNet ImageNet-Subset\nT= 5\nFeTrIL166.3 54.8 72.2\nFeTrIL565.7 53.8 72.2\nFeTrIL1065.1 53.8 71.6\nFeTrILherd66.2 53.8 72.1\nFeTrILrand65.1 51.5 70.3\nTable 3. Average top-1 CIL accuracy obtained with the variants\nof pseudo-feature selection from Subsection 3.2 for T= 5. We\nsetk={1,5,10}for the similarity rank between the past and\nnew classes to test the effect of class similarities. There are 10\n(CIFAR-100 and ImageNet-Subset) and 20 (TinyImageNet) new\nclasses per state from which to select features translation.\nThe results from Table 3 motivate the use of FeTrIL1in\nthe main experiments. Overall, the geometric translation to-\nward the centroid of the past class is by far more important\nthan the new classes features sampling policy. This finding\nis also supported by the results obtained with a single new\nclass per CIL state (Table 1).\nMemory footprint. A low memory footprint is a de-\nsirable property of incremental learning algorithms be-\ncause they are most useful in memory-constrained appli-\ncations [25, 33, 34], and recommended for embedded de-\nvices [10]. All EFCIL methods need to store a representa-\ntion of past classes to counter catastrophic forgetting. Nat-\nurally, this representation should be as compact as possible.\nMainstream methods (such as LwF-MC [20], PASS [49],\nIL2A [49], and SSRE [50]) need to the previous and current\ndeep models during CIL updates for distillation. ResNet-\n18 [12], the most frequent CIL backbone, has approxi-\nmately 11.4M parameters. Consequently, distillation-based\nmethods require around 22.8M parameters. Transfer-based\nmethods, such as DeeSIL [1] and FeTrIL, use only the deep\nmodel learned in the initial state and frozen afterwards, and\nonly need 11.4M parameters for the model. DeeSIL does\nnot need supplementary parameters during incremental up-\ndates. However, this comes at the cost of poor global dis-\ncrimination of classes, which is reflected in the final per-\nformance. FeTrIL stores the class centroids of past classes\nin order to perform feature translation. Each class needs\n512 parameters, which leads to a supplementary 51.2K\n012345678910\nIncremental state20%40%60%80%100%\nImageNet-Subset, T=10\nova - 71.2\nr=25 - 70.8r=10 - 70.0\nr=1 - 67.3ova - 71.2\nr=25 - 70.8r=10 - 70.0\nr=1 - 67.3Figure 4. Top-1 incremental accuracy of FeTrIL1for approximate\ntraining of the classification layer with different ratios for negative\nsampling. ova denotes a classical one-vs-all training procedure\nwhich is used to report the main results from Table 1 and Figure 3.\nand 102.4 memory need for 100 and 200 classes, respec-\ntively. The class similarities needed for pseudo-feature se-\nlection (Subsection 3.2) can be computed sequentially and\nthe added memory cost of this step is negligible. PASS [49],\nIL2A [48] and SSRE [50] also require the storage of a proto-\ntype (mean representation) for each past class and their foot-\nprint is equivalent to that of FeTrIL. IL2A [48] addition-\nally stores a covariance matrix per past class (512x512 for\nResNet-18) for optimal functioning, which is prohibitive.\nComplexity of incremental updates. CIL is useful in\nresource-constrained environments, and the integration of\nnew classes should be fast [10, 33]. Distillation-based meth-\nods retrain the full backbone model at each update. This is\ncostly because backpropagation complexity depends on the\nnetwork architecture, the number of samples and the num-\nber of epochs [8]. Updates of transfer-based methods are\nsimpler because they update only the final layer. DeeSIL\ntrains linear classifiers using a one-vs-all procedure within\neach CIL state. The complexity of one training epoch for\nall classifiers in a CIL state is O((n\nT)2sd)[3], with n- to-\ntal number of classes in the dataset, d- dimensionality of\nfeatures and s- samples per class. FeTrIL retrains all lin-\near classifiers, past and new, in each CIL state to improve\nglobal separability. Its complexity is O(n2sd)in the last\nincremental state, which includes all classes. However, the\none-versus-all training can be replaced with a one-versus-\nmany training with negligible loss of accuracy. A sampling\nof negative features is performed to respect a predefined\nratiorbetween negatives and positives used to train each012345678910\nIncremental state0%25%50%75%100%T op-1 accuracy\nPast\nNewAvgSSRE\nPast\nNewAvg\n012345678910\nIncremental state\nFeTrILTinyImageNet, T=10Figure 5. Top-1 incremental accuracy per state for past and new\nclasses for TinyImageNet, with T= 10 incremental states for\nFeTrIL1and SSRE, the best compared method. An ideal method\nwould provide high accuracy, but also similar performance for past\nand new classes. The accuracy of past and new classes is globally\ncloser for FeTrIL1, which indicates that our method provides a\nbetter stability-plasticity balance than SSRE. Overall accuracy is\nbetter for FeTrIL1in Figure 3 because the contribution of new\nclasses in each state diminishes during the CIL process.\nclassifier. This approximation has O(rnsd)complexity. It\nis interesting since r < n , and is more and more useful as n\ngrows during the IL process since rremains constant.\nIn Figure 4, we present results with different rvalues for\nImageNet-Subset, T= 10 . Accuracy drops when negative\nsampling is performed, but it is close to that of one-vs-all\ntraining when r= 25 andr= 10 . Performance drops\nmore significantly for r= 1, when each linear classifier is\nlearned with an aggressive sampling of negatives. Similar\nresults for CIFAR-100 and TinyImageNet are provided in\nthe suppl. material. Globally, Figure 4 indicates that FeTrIL\nincrements can be accelerated with little accuracy loss.\nWe measure the time needed for incremental training of\nImageNet-Subset, T= 10 . The training of the initial model\nis similar for all models and is thus discarded. FeTrIL train-\ning is done on a single thread of an Intel E5-2620v4 CPU,\nand only takes 1 hour, 4 minutes and 16 seconds. If FeTrIL\nis run with r= 10 ratio between positives and negatives,\ntraining time is only 15 minutes and 3 seconds. In com-\nparison, PASS [49] needs 11 hours, 8 minutes and 19 sec-\nonds on an NVIDIA V100 GPU, with 4 workers for data\nloading. While clearly favorable to FeTrIL, the comparison\nis biased in favor of PASS since this method uses an en-\ntire GPU, in comparison to a single CPU thread for FeTrIL.\nFurther speed gains are possible for our method by using a\nGPU implementation of the linear layer. Our method would\nrun much faster with a GPU implementation of the linear\nlayer. Note that the running time of the other methods, such\nas LUCIR [14] and SSRE [50], which perform backpropa-\ngation is similar to that of PASS [49].\nStability-plasticity balance. CIL should ideally ensure\na similar accuracy level for past and new classes [25, 50].\nFigure 5 shows that the two methods have complementary\nbehavior, which results from the way deep backbones are\nused. SSRE is biased toward new classes since the model\nis fine tuned in each incremental state. FeTrIL favors pastclasses because the deep model is learned with the initial\nclasses (a subset of past classes) and then frozen. The accu-\nracy gap between past and new classes is smaller for FeTrIL\ncompared to SSRE, except for state 4. There, low perfor-\nmance on new classes is probably explained by a strong do-\nmain shift compared to the initial state. Globally, the pro-\nposed method improves the stability-plasticity balance.\n5. Conclusion\nWe introduce FeTrIL, a new method which addresses\nexemplar-free class-incremental learning. The proposed\ncombination of a frozen feature extractor and of a pseudo-\nfeature generator improves results compared to recent EF-\nCIL methods. The generation of pseudo-features is sim-\nple, since it consists in a geometric translation, yet effec-\ntive. Our proposal is advantageous from memory and speed\nperspectives compared to mainstream methods [14, 34, 39,\n43, 46, 48, 49, 50]. This is particularly important for edge\ndevices [10, 33], whose storage and computation capaci-\nties are limited. FeTrIL performance is also close to that\nof exemplar-based methods, which need to store samples of\npast classes to mitigate catastrophic forgetting. While a gap\nbetween exemplar-based and exemplar-free setting subsists,\nit becomes significantly narrower. The results reported here\nresonate with past works which show that simple methods\ncan be highly effective in CIL [2, 25, 32]. They question the\nusefulness of the knowledge distillation component, used\nby a majority of existing methods. The FeTrIL code will be\nmade public to enable reproducibility.\nThe main limitations of the proposed method motivate\nour future work. First, FeTrIL uses a frozen feature ex-\ntractor learned on the initial state and tends to favor past\nclasses over new ones. We will investigate ways to combine\nthe pseudo-feature generation mechanism and fine-tuning to\nfurther improve global performance, as well as the stability-\nplasticity balance. Second, FeTrIL produces usable pseudo-\nfeatures, but past class representations would be better if the\npseudo-features would be more similar to the original fea-\ntures of past classes. We will study methods that generate\nmore refined features, for instance by using the distribution\nof the initial features. Last but not least, the tested selection\nstrategies are all effective. However, they could be further\nimproved by filtering out outliers based on the localization\nof pseudo-features in the representation space.\nAcknowledgements . This work was supported by the Eu-\nropean Commission under European Horizon 2020 Pro-\ngramme, grant number 951911 - AI4Media. It was made\npossible by the use of the FactoryIA supercomputer, finan-\ncially supported by the Ile-de-France Regional Council.References\n[1] Eden Belouadah and Adrian Popescu. Deesil: Deep-shallow\nincremental learning. TaskCV Workshop @ ECCV 2018. ,\n2018.\n[2] Eden Belouadah, Adrian Popescu, and Ioannis Kanellos.\nA comprehensive study of class incremental learning algo-\nrithms for visual tasks. Neural Networks , 135:38\u201354, 2021.\n[3] L \u00b4eon Bottou and Olivier Bousquet. The tradeoffs of large\nscale learning. Advances in neural information processing\nsystems , 20, 2007.\n[4] Francisco M. Castro, Manuel J. Mar \u00b4\u0131n-Jim \u00b4enez, Nicol \u00b4as\nGuil, Cordelia Schmid, and Karteek Alahari. End-to-end in-\ncremental learning. In Computer Vision - ECCV 2018 - 15th\nEuropean Conference, Munich, Germany, September 8-14,\n2018, Proceedings, Part XII , pages 241\u2013257, 2018.\n[5] Debasmit Das and CS George Lee. A two-stage approach to\nfew-shot learning for image recognition. IEEE Transactions\non Image Processing , 29:3336\u20133350, 2019.\n[6] Akshay Raj Dhamija, Touqeer Ahmad, Jonathan Schwan,\nMohsen Jafarzadeh, Chunchun Li, and Terrance E Boult.\nSelf-supervised features improve open-world learning. arXiv\npreprint arXiv:2102.07848 , 2021.\n[7] Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas\nRobert, and Eduardo Valle. Podnet: Pooled outputs dis-\ntillation for small-tasks incremental learning. In Com-\nputer vision-ECCV 2020-16th European conference, Glas-\ngow, UK, August 23-28, 2020, Proceedings, Part XX , volume\n12365, pages 86\u2013102. Springer, 2020.\n[8] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep\nlearning . MIT press, 2016.\n[9] Tyler L Hayes and Christopher Kanan. Lifelong machine\nlearning with deep streaming linear discriminant analysis.\nInProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition Workshops , pages 220\u2013221,\n2020.\n[10] Tyler L Hayes and Christopher Kanan. Online con-\ntinual learning for embedded devices. arXiv preprint\narXiv:2203.10681 , 2022.\n[11] Chen He, Ruiping Wang, Shiguang Shan, and Xilin Chen.\nExemplar-supported generative reproduction for class in-\ncremental learning. In British Machine Vision Conference\n2018, BMVC 2018, Northumbria University, Newcastle, UK,\nSeptember 3-6, 2018 , page 98, 2018.\n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Conference\non Computer Vision and Pattern Recognition , CVPR, 2016.\n[13] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.\nDistilling the knowledge in a neural network. CoRR ,\nabs/1503.02531, 2015.\n[14] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and\nDahua Lin. Learning a unified classifier incrementally via re-\nbalancing. In IEEE Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2019, Long Beach, CA, USA, June\n16-20, 2019 , pages 831\u2013839, 2019.\n[15] Ronald Kemker, Marc McClure, Angelina Abitino, Tyler\nHayes, and Christopher Kanan. Measuring catastrophic for-getting in neural networks. In Proceedings of the AAAI Con-\nference on Artificial Intelligence , volume 32, 2018.\n[16] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel\nVeness, Guillaume Desjardins, Andrei A Rusu, Kieran\nMilan, John Quan, Tiago Ramalho, Agnieszka Grabska-\nBarwinska, et al. Overcoming catastrophic forgetting in neu-\nral networks. Proceedings of the national academy of sci-\nences , 114(13):3521\u20133526, 2017.\n[17] Alex Krizhevsky. Learning multiple layers of features from\ntiny images. Technical report, University of Toronto, 2009.\n[18] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah\nParisot, Xu Jia, Ales Leonardis, Gregory G. Slabaugh, and\nTinne Tuytelaars. Continual learning: A comparative study\non how to defy forgetting in classification tasks. CoRR ,\nabs/1909.08383, 2019.\n[19] Ya Le and Xuan Yang. Tiny imagenet visual recognition\nchallenge. CS 231N , 7(7):3, 2015.\n[20] Zhizhong Li and Derek Hoiem. Learning without forgetting.\nInEuropean Conference on Computer Vision , ECCV , 2016.\n[21] Yu Liu, Sarah Parisot, Gregory Slabaugh, Xu Jia, Ales\nLeonardis, and Tinne Tuytelaars. More classifiers, less for-\ngetting: A generic multi-classifier paradigm for incremen-\ntal learning. In European Conference on Computer Vision ,\npages 699\u2013716. Springer, 2020.\n[22] Yaoyao Liu, Bernt Schiele, and Qianru Sun. Adaptive aggre-\ngation networks for class-incremental learning. In Confer-\nence on Computer Vision and Pattern Recognition , CVPR,\n2021.\n[23] Yaoyao Liu, Yuting Su, An-An Liu, Bernt Schiele, and\nQianru Sun. Mnemonics training: Multi-class incremental\nlearning without forgetting. In 2020 IEEE/CVF Conference\non Computer Vision and Pattern Recognition, CVPR 2020,\nSeattle, WA, USA, June 13-19, 2020 , pages 12242\u201312251.\nIEEE, 2020.\n[24] Yaoyao Liu, Yuting Su, An-An Liu, Bernt Schiele, and\nQianru Sun. Mnemonics training: Multi-class incremen-\ntal learning without forgetting. In The IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) , 06 2020.\n[25] Marc Masana, Xialei Liu, Bartlomiej Twardowski, Mikel\nMenta, Andrew D. Bagdanov, and Joost van de Weijer.\nClass-incremental learning: survey and performance evalu-\nation on image classification, 2021.\n[26] Michael Mccloskey and Neil J. Cohen. Catastrophic in-\nterference in connectionist networks: The sequential learn-\ning problem. The Psychology of Learning and Motivation ,\n24:104\u2013169, 1989.\n[27] Thomas Mensink, Jakob Verbeek, Florent Perronnin, and\nGabriela Csurka. Distance-based image classification: Gen-\neralizing to new classes at near-zero cost. IEEE transactions\non pattern analysis and machine intelligence , 35(11):2624\u2013\n2637, 2013.\n[28] M Mermillod, A Bugaiska, and P Bonin. The stability-\nplasticity dilemma: investigating the continuum from catas-\ntrophic forgetting to age-limited learning effects. Frontiers\nin Psychology , 4:504\u2013504, 2013.\n[29] Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang.\nWhat is being transferred in transfer learning? arXiv preprint\narXiv:2008.11687 , 2020.[30] German Ignacio Parisi, Ronald Kemker, Jose L. Part,\nChristopher Kanan, and Stefan Wermter. Continual lifelong\nlearning with neural networks: A review. Neural Networks ,\n113, 2019.\n[31] Fabian Pedregosa, Ga \u00a8el Varoquaux, Alexandre Gramfort,\nVincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu\nBlondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg,\nJake VanderPlas, Alexandre Passos, David Cournapeau,\nMatthieu Brucher, Matthieu Perrot, and Edouard Duches-\nnay. Scikit-learn: Machine learning in python. CoRR ,\nabs/1201.0490, 2012.\n[32] Ameya Prabhu, Philip HS Torr, and Puneet K Dokania.\nGdumb: A simple approach that questions our progress in\ncontinual learning. In European Conference on Computer\nVision , pages 524\u2013540. Springer, 2020.\n[33] Leonardo Ravaglia, Manuele Rusci, Davide Nadalini,\nAlessandro Capotondi, Francesco Conti, and Luca Benini. A\ntinyml platform for on-device continual learning with quan-\ntized latent replays. IEEE Journal on Emerging and Selected\nTopics in Circuits and Systems , 11(4):789\u2013802, 2021.\n[34] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg\nSperl, and Christoph H. Lampert. icarl: Incremental classi-\nfier and representation learning. In Conference on Computer\nVision and Pattern Recognition , CVPR, 2017.\n[35] Ethan M Rudd, Lalit P Jain, Walter J Scheirer, and Ter-\nrance E Boult. The extreme value machine. IEEE\ntransactions on pattern analysis and machine intelligence ,\n40(3):762\u2013768, 2017.\n[36] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpa-\nthy, Aditya Khosla, Michael S. Bernstein, Alexander C.\nBerg, and Fei-Fei Li. Imagenet large scale visual recogni-\ntion challenge. International Journal of Computer Vision ,\n115(3):211\u2013252, 2015.\n[37] Jeffrey C Schlimmer and Douglas Fisher. A case study of\nincremental concept induction. In AAAI , volume 86, pages\n496\u2013501, 1986.\n[38] Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan,\nand Stefan Carlsson. Cnn features off-the-shelf: an astound-\ning baseline for recognition. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition work-\nshops , pages 806\u2013813, 2014.\n[39] James Smith, Yen-Chang Hsu, Jonathan Balloch, Yilin Shen,\nHongxia Jin, and Zsolt Kira. Always be dreaming: A new\napproach for data-free class-incremental learning. arXiv\npreprint arXiv:2106.09701 , 2021.\n[40] Chuanqi Tan, Fuchun Sun, Tao Kong, Wenchang Zhang,\nChao Yang, and Chunfang Liu. A survey on deep transfer\nlearning. In International conference on artificial neural net-\nworks , pages 270\u2013279. Springer, 2018.\n[41] Gido M Van de Ven and Andreas S Tolias. Three scenar-\nios for continual learning. arXiv preprint arXiv:1904.07734 ,\n2019.\n[42] Ragav Venkatesan, Hemanth Venkateswara, Sethuraman\nPanchanathan, and Baoxin Li. A strategy for an\nuncompromising incremental learner. arXiv preprint\narXiv:1705.00744 , 2017.[43] Vinay Kumar Verma, Kevin J. Liang, Nikhil Mehta, Piyush\nRai, and Lawrence Carin. Efficient feature transformations\nfor discriminative and generative continual learning. CoRR ,\nabs/2103.13558, 2021.\n[44] Max Welling. Herding dynamical weights to learn. In Pro-\nceedings of the 26th Annual International Conference on\nMachine Learning, ICML 2009, Montreal, Quebec, Canada,\nJune 14-18, 2009 , pages 1121\u20131128, 2009.\n[45] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye,\nZicheng Liu, Yandong Guo, and Yun Fu. Large scale in-\ncremental learning. In IEEE Conference on Computer Vi-\nsion and Pattern Recognition, CVPR 2019, Long Beach, CA,\nUSA, June 16-20, 2019 , pages 374\u2013382, 2019.\n[46] Lu Yu, Bartlomiej Twardowski, Xialei Liu, Luis Herranz,\nKai Wang, Yongmei Cheng, Shangling Jui, and Joost van de\nWeijer. Semantic drift compensation for class-incremental\nlearning. In 2020 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, CVPR 2020, Seattle, WA, USA,\nJune 13-19, 2020 , pages 6980\u20136989. IEEE, 2020.\n[47] Bowen Zhao, Xi Xiao, Guojun Gan, Bin Zhang, and Shu-Tao\nXia. Maintaining discrimination and fairness in class incre-\nmental learning. In 2020 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, CVPR 2020, Seattle,\nWA, USA, June 13-19, 2020 , pages 13205\u201313214. IEEE,\n2020.\n[48] Fei Zhu, Zhen Cheng, Xu-yao Zhang, and Cheng-lin Liu.\nClass-incremental learning via dual augmentation. Advances\nin Neural Information Processing Systems , 34, 2021.\n[49] Fei Zhu, Xu-Yao Zhang, Chuang Wang, Fei Yin, and Cheng-\nLin Liu. Prototype augmentation and self-supervision for\nincremental learning. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , pages\n5871\u20135880, 2021.\n[50] Kai Zhu, Wei Zhai, Yang Cao, Jiebo Luo, and Zheng-\nJun Zha. Self-sustaining representation expansion for non-\nexemplar class-incremental learning. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 9296\u20139305, 2022."
}